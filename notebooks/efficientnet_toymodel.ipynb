{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\" # you can also use tensorflow or torch\n",
    "\n",
    "import keras_cv\n",
    "import keras\n",
    "from keras import ops\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import ops\n",
    "import math\n",
    "\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"Keras:\", keras.__version__)\n",
    "print(\"KerasCV:\", keras_cv.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Config</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    verbose = 1  # Verbosity\n",
    "    seed = 42  # Random seed\n",
    "    preset = \"efficientnetv2_b2_imagenet\"  # Name of pretrained classifier\n",
    "    image_size = [224, 224]  # Input image size\n",
    "    epochs = 1 # 12 # Training epochs\n",
    "    batch_size = 32 # 96  # Batch size\n",
    "    lr_mode = \"step\" # LR scheduler mode from one of \"cos\", \"step\", \"exp\"\n",
    "    drop_remainder = True  # Drop incomplete batches\n",
    "    num_classes = 6 # Number of classes in the dataset\n",
    "    num_folds = 5 # Number of folds to split the dataset\n",
    "    fold = 0 # Which fold to set as validation data\n",
    "    class_names = ['X4_mean', 'X11_mean', 'X18_mean',\n",
    "                   'X26_mean', 'X50_mean', 'X3112_mean',]\n",
    "    aux_class_names = list(map(lambda x: x.replace(\"mean\",\"sd\"), class_names))\n",
    "    num_classes = len(class_names)\n",
    "    aux_num_classes = len(aux_class_names)\n",
    "\n",
    "BASE_PATH = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reproducibility</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load and process data set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train + Valid\n",
    "df = pd.read_csv(f'{BASE_PATH}/train.csv')\n",
    "df['image_path'] = f'{BASE_PATH}/train_images/'+df['id'].astype(str)+'.jpeg'\n",
    "df.loc[:, CFG.aux_class_names] = df.loc[:, CFG.aux_class_names].fillna(-1)\n",
    "display(df.head(2))\n",
    "\n",
    "# Test\n",
    "test_df = pd.read_csv(f'{BASE_PATH}/test.csv')\n",
    "test_df['image_path'] = f'{BASE_PATH}/test_images/'+test_df['id'].astype(str)+'.jpeg'\n",
    "FEATURE_COLS = test_df.columns[1:-1].tolist()\n",
    "display(test_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_augmenter():\n",
    "    # Define augmentations\n",
    "    aug_layers = [\n",
    "        keras_cv.layers.RandomBrightness(factor=0.1, value_range=(0, 1)),\n",
    "        keras_cv.layers.RandomContrast(factor=0.1, value_range=(0, 1)),\n",
    "        keras_cv.layers.RandomSaturation(factor=(0.45, 0.55)),\n",
    "        keras_cv.layers.RandomHue(factor=0.1, value_range=(0, 1)),\n",
    "        keras_cv.layers.RandomCutout(height_factor=(0.06, 0.15), width_factor=(0.06, 0.15)),\n",
    "        keras_cv.layers.RandomFlip(mode=\"horizontal_and_vertical\"),\n",
    "        keras_cv.layers.RandomZoom(height_factor=(0.05, 0.15)),\n",
    "        keras_cv.layers.RandomRotation(factor=(0.01, 0.05)),\n",
    "    ]\n",
    "    \n",
    "    # Apply augmentations to random samples\n",
    "    aug_layers = [keras_cv.layers.RandomApply(x, rate=0.5) for x in aug_layers]\n",
    "    \n",
    "    # Build augmentation layer\n",
    "    augmenter = keras_cv.layers.Augmenter(aug_layers)\n",
    "\n",
    "    # Apply augmentations\n",
    "    def augment(inp, label):\n",
    "        images = inp[\"images\"]\n",
    "        aug_data = {\"images\": images}\n",
    "        aug_data = augmenter(aug_data)\n",
    "        inp[\"images\"] = aug_data[\"images\"]\n",
    "        return inp, label\n",
    "    return augment\n",
    "\n",
    "\n",
    "def build_decoder(with_labels=True, target_size=CFG.image_size):\n",
    "    def decode_image(inp):\n",
    "        path = inp[\"images\"]\n",
    "        \n",
    "        # Read jpeg image\n",
    "        file_bytes = tf.io.read_file(path)\n",
    "        image = tf.io.decode_jpeg(file_bytes)\n",
    "        \n",
    "        # Resize\n",
    "        image = tf.image.resize(image, size=target_size, method=\"area\")\n",
    "        \n",
    "        # Rescale image\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255.0\n",
    "        \n",
    "        # Reshape\n",
    "        image = tf.reshape(image, [*target_size, 3])\n",
    "        \n",
    "        inp[\"images\"] = image\n",
    "        return inp\n",
    "\n",
    "    def decode_label(label, num_classes):\n",
    "        label = tf.cast(label, tf.float32)\n",
    "        label = tf.reshape(label, [num_classes])\n",
    "        return label\n",
    "\n",
    "    def decode_with_labels(inp, labels=None):\n",
    "        inp = decode_image(inp)\n",
    "        label = decode_label(labels[0], CFG.num_classes)\n",
    "        aux_label = decode_label(labels[1], CFG.aux_num_classes)\n",
    "        return (inp, (label, aux_label))\n",
    "\n",
    "    return decode_with_labels if with_labels else decode_image\n",
    "\n",
    "\n",
    "def build_dataset(\n",
    "    paths,\n",
    "    features,\n",
    "    labels=None,\n",
    "    aux_labels=None,\n",
    "    batch_size=32,\n",
    "    cache=True,\n",
    "    decode_fn=None,\n",
    "    augment_fn=None,\n",
    "    augment=False,\n",
    "    repeat=True,\n",
    "    shuffle=1024,\n",
    "    cache_dir=\"\",\n",
    "    drop_remainder=False,\n",
    "):\n",
    "    if cache_dir != \"\" and cache is True:\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    if decode_fn is None:\n",
    "        decode_fn = build_decoder(labels is not None or aux_labels is not None)\n",
    "\n",
    "    if augment_fn is None:\n",
    "        augment_fn = build_augmenter()\n",
    "\n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    inp = {\"images\": paths, \"features\": features}\n",
    "    slices = (inp, (labels, aux_labels)) if labels is not None else inp\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(slices)\n",
    "    ds = ds.map(decode_fn, num_parallel_calls=AUTO)\n",
    "    ds = ds.cache(cache_dir) if cache else ds\n",
    "    ds = ds.repeat() if repeat else ds\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle, seed=CFG.seed)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        ds = ds.with_options(opt)\n",
    "    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n",
    "    ds = ds.map(augment_fn, num_parallel_calls=AUTO) if augment else ds\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=CFG.num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Create separate bin for each traits\n",
    "for i, trait in enumerate(CFG.class_names):\n",
    "\n",
    "    # Determine the bin edges dynamically based on the distribution of traits\n",
    "    bin_edges = np.percentile(df[trait], np.linspace(0, 100, CFG.num_folds + 1))\n",
    "    df[f\"bin_{i}\"] = np.digitize(df[trait], bin_edges)\n",
    "\n",
    "# Concatenate the bins into a final bin\n",
    "df[\"final_bin\"] = (\n",
    "    df[[f\"bin_{i}\" for i in range(len(CFG.class_names))]]\n",
    "    .astype(str)\n",
    "    .agg(\"\".join, axis=1)\n",
    ")\n",
    "\n",
    "# Perform the stratified split using final bin\n",
    "df = df.reset_index(drop=True)\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(df, df[\"final_bin\"])):\n",
    "    df.loc[valid_idx, \"fold\"] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from full data\n",
    "sample_df = df.copy()\n",
    "train_df = sample_df[sample_df.fold != CFG.fold]\n",
    "valid_df = sample_df[sample_df.fold == CFG.fold]\n",
    "print(f\"# Num Train: {len(train_df)} | Num Valid: {len(valid_df)}\")\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_df[FEATURE_COLS].values)\n",
    "valid_features = scaler.transform(valid_df[FEATURE_COLS].values)\n",
    "\n",
    "# Train\n",
    "train_paths = train_df.image_path.values\n",
    "train_labels = train_df[CFG.class_names].values\n",
    "train_aux_labels = train_df[CFG.aux_class_names].values\n",
    "train_ds = build_dataset(train_paths, train_features, train_labels, train_aux_labels,\n",
    "                         batch_size=CFG.batch_size,\n",
    "                         repeat=True, shuffle=True, augment=True, cache=False)\n",
    "\n",
    "# Valid\n",
    "valid_paths = valid_df.image_path.values\n",
    "valid_labels = valid_df[CFG.class_names].values\n",
    "valid_aux_labels = valid_df[CFG.aux_class_names].values\n",
    "valid_ds = build_dataset(valid_paths, valid_features, valid_labels, valid_aux_labels,\n",
    "                         batch_size=CFG.batch_size,\n",
    "                         repeat=False, shuffle=False, augment=False, cache=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inps, tars = next(iter(train_ds))\n",
    "imgs = inps[\"images\"]\n",
    "num_imgs, num_cols = 8, 4\n",
    "\n",
    "plt.figure(figsize=(4 * num_cols, num_imgs // num_cols * 5))\n",
    "for i, (img, tar) in enumerate(zip(imgs[:num_imgs], tars[0][:num_imgs])):\n",
    "    plt.subplot(num_imgs // num_cols, num_cols, i + 1)\n",
    "    img = img. numpy()\n",
    "    tar = tar.numpy()\n",
    "    \n",
    "    img = (img - img.min()) / (img.max() + 1e-4)\n",
    "\n",
    "    formatted_tar = \"\\n\".join(\n",
    "        [\n",
    "            \", \".join(\n",
    "                f\"{name.replace('_mean','')}: {val:.2f}\"\n",
    "                for name, val in zip(CFG.class_names[j : j + 3], tar[j : j + 3])\n",
    "            )\n",
    "            for j in range(0, len(CFG.class_names), 3)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"[{formatted_tar}]\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define loss</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R2Loss(keras.losses.Loss):\n",
    "    def __init__(self, use_mask=False, name=\"r2_loss\"):\n",
    "        super().__init__(name=name)\n",
    "        self.use_mask = use_mask\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        if self.use_mask:\n",
    "            mask = (y_true != -1)\n",
    "            y_true = ops.where(mask, y_true, 0.0)\n",
    "            y_pred = ops.where(mask, y_pred, 0.0)\n",
    "        SS_res = ops.sum(ops.square(y_true - y_pred), axis=0)  # (B, C) -> (C,)\n",
    "        SS_tot = ops.sum(ops.square(y_true - ops.mean(y_true, axis=0)), axis=0)  # (B, C) -> (C,)\n",
    "        r2_loss = SS_res / (SS_tot + 1e-6)  # (C,)\n",
    "        return ops.mean(r2_loss)  # ()\n",
    "    \n",
    "class R2Metric(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"r2\", **kwargs):\n",
    "        super(R2Metric, self).__init__(name=name, **kwargs)\n",
    "        self.SS_res = self.add_weight(name='SS_res', shape=(6,), initializer='zeros')\n",
    "        self.SS_tot = self.add_weight(name='SS_tot', shape=(6,) ,initializer='zeros')\n",
    "        self.num_samples = self.add_weight(name='num_samples', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        SS_res = ops.sum(ops.square(y_true - y_pred), axis=0)\n",
    "        SS_tot = ops.sum(ops.square(y_true - ops.mean(y_true, axis=0)), axis=0)\n",
    "        self.SS_res.assign_add(SS_res)\n",
    "        self.SS_tot.assign_add(SS_tot)\n",
    "        self.num_samples.assign_add(ops.cast(ops.shape(y_true)[0], \"float32\"))\n",
    "\n",
    "    def result(self):\n",
    "        r2 = 1 - self.SS_res / (self.SS_tot + 1e-6)\n",
    "        return ops.mean(r2)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.total_SS_res.assign(0)\n",
    "        self.total_SS_tot.assign(0)\n",
    "        self.num_samples.assign(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Construct model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input layers\n",
    "img_input = keras.Input(shape=(*CFG.image_size, 3), name=\"images\")\n",
    "feat_input = keras.Input(shape=(len(FEATURE_COLS),), name=\"features\")\n",
    "\n",
    "# Branch for image input\n",
    "backbone = keras_cv.models.EfficientNetV2Backbone.from_preset(CFG.preset)\n",
    "x1 = backbone(img_input)\n",
    "x1 = keras.layers.GlobalAveragePooling2D()(x1)\n",
    "x1 = keras.layers.Dropout(0.2)(x1)\n",
    "\n",
    "# Branch for tabular/feature input\n",
    "x2 = keras.layers.Dense(326, activation=\"selu\")(feat_input)\n",
    "x2 = keras.layers.Dense(64, activation=\"selu\")(x2)\n",
    "x2 = keras.layers.Dropout(0.1)(x2)\n",
    "\n",
    "# Concatenate both branches\n",
    "concat = keras.layers.Concatenate()([x1, x2])\n",
    "\n",
    "# Output layer\n",
    "out1 = keras.layers.Dense(CFG.num_classes, activation=None, name=\"head\")(concat)\n",
    "out2 = keras.layers.Dense(CFG.aux_num_classes, activation=\"relu\", name=\"aux_head\")(concat)\n",
    "out = {\"head\": out1, \"aux_head\":out2}\n",
    "\n",
    "# Build model\n",
    "model = keras.models.Model([img_input, feat_input], out)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss={\n",
    "        \"head\": R2Loss(use_mask=False),\n",
    "        \"aux_head\": R2Loss(use_mask=True), # use_mask to ignore `NaN` auxiliary labels\n",
    "    },\n",
    "    loss_weights={\"head\": 1.0, \"aux_head\": 0.3},  # more weight to main task\n",
    "    metrics={\"head\": R2Metric()}, # evaluation metric only on main task\n",
    ")\n",
    "\n",
    "# Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LR scheduler</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):\n",
    "    lr_start, lr_max, lr_min = 5e-5, 8e-6 * batch_size, 1e-5\n",
    "    lr_ramp_ep, lr_sus_ep, lr_decay = 3, 0, 0.75\n",
    "\n",
    "    def lrfn(epoch):  # Learning rate update function\n",
    "        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n",
    "        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n",
    "        elif mode == 'cos':\n",
    "            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n",
    "            phase = math.pi * decay_epoch_index / decay_total_epochs\n",
    "            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n",
    "        return lr\n",
    "\n",
    "    if plot:  # Plot lr curve if plot is True\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n",
    "        plt.xlabel('epoch'); plt.ylabel('lr')\n",
    "        plt.title('LR Scheduler')\n",
    "        plt.show()\n",
    "\n",
    "    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cb = get_lr_callback(CFG.batch_size, mode=CFG.lr_mode, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_cb = keras.callbacks.ModelCheckpoint(\n",
    "    \"best_model.keras\",\n",
    "    monitor=\"val_head_r2\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode=\"max\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=CFG.epochs,\n",
    "    callbacks=[lr_cb, ckpt_cb],\n",
    "    steps_per_epoch=len(train_df) // CFG.batch_size,\n",
    "    validation_data=valid_ds,\n",
    "    verbose=CFG.verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Results</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Result\n",
    "best_R2 = max(history.history['val_head_r2'])\n",
    "best_Epoch = np.argmax(history.history['val_head_r2']) + 1\n",
    "print(\"#\" * 10 + \" Result \" + \"#\" * 10)\n",
    "print(f\"Best R2: {best_R2:.5f}\")\n",
    "print(f\"Best Epoch: {best_Epoch}\")\n",
    "print(\"#\" * 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"best_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test_paths = test_df.image_path.values\n",
    "test_features = scaler.transform(test_df[FEATURE_COLS].values) \n",
    "test_ds = build_dataset(test_paths, test_features, batch_size=CFG.batch_size,\n",
    "                         repeat=False, shuffle=False, augment=False, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_ds)[\"head\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = test_df[[\"id\"]].copy()\n",
    "target_cols = [x.replace(\"_mean\",\"\") for x in CFG.class_names]\n",
    "pred_df[target_cols] = preds.tolist()\n",
    "\n",
    "pred_df.to_csv(\"submission.csv\", index=False)\n",
    "pred_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
