{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "\n",
    "import keras_cv\n",
    "import keras\n",
    "from keras import ops\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"Keras:\", keras.__version__)\n",
    "print(\"KerasCV:\", keras_cv.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Config</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    verbose = 1  # Verbosity\n",
    "    seed = 42  # Random seed\n",
    "    preset = \"efficientnetv2_b2_imagenet\"  # Name of pretrained classifier\n",
    "    image_size = [224, 224]  # Input image size\n",
    "    epochs = 1 # 12 # Training epochs\n",
    "    batch_size = 32 # 96  # Batch size\n",
    "    lr_mode = \"step\" # LR scheduler mode from one of \"cos\", \"step\", \"exp\"\n",
    "    drop_remainder = True  # Drop incomplete batches\n",
    "    num_classes = 6 # Number of classes in the dataset\n",
    "    num_folds = 5 # Number of folds to split the dataset\n",
    "    fold = 0 # Which fold to set as validation data\n",
    "    class_names = ['X4_mean', 'X11_mean', 'X18_mean',\n",
    "                   'X26_mean', 'X50_mean', 'X3112_mean',]\n",
    "    aux_class_names = list(map(lambda x: x.replace(\"mean\",\"sd\"), class_names))\n",
    "    num_classes = len(class_names)\n",
    "    aux_num_classes = len(aux_class_names)\n",
    "    retrain_classifier = True\n",
    "    perform_cv = True\n",
    "\n",
    "BASE_PATH = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reproducibility</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load and process data set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Train + Valid\n",
    "df = pd.read_csv(f'{BASE_PATH}/train.csv')\n",
    "df['image_path'] = f'{BASE_PATH}/train_images/'+df['id'].astype(str)+'.jpeg'\n",
    "df.loc[:, CFG.aux_class_names] = df.loc[:, CFG.aux_class_names].fillna(-1)\n",
    "display(df.head(2))\n",
    "\n",
    "# Test\n",
    "test_df = pd.read_csv(f'{BASE_PATH}/test.csv')\n",
    "test_df['image_path'] = f'{BASE_PATH}/test_images/'+test_df['id'].astype(str)+'.jpeg'\n",
    "FEATURE_COLS = test_df.columns[1:-1].tolist()\n",
    "display(test_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def build_augmenter():\n",
    "    # Define augmentations\n",
    "    aug_layers = [\n",
    "        keras_cv.layers.RandomBrightness(factor=0.1, value_range=(0, 1)),\n",
    "        keras_cv.layers.RandomContrast(factor=0.1, value_range=(0, 1)),\n",
    "        keras_cv.layers.RandomSaturation(factor=(0.45, 0.55)),\n",
    "        keras_cv.layers.RandomHue(factor=0.1, value_range=(0, 1)),\n",
    "        keras_cv.layers.RandomCutout(height_factor=(0.06, 0.15), width_factor=(0.06, 0.15)),\n",
    "        keras_cv.layers.RandomFlip(mode=\"horizontal_and_vertical\"),\n",
    "        keras_cv.layers.RandomZoom(height_factor=(0.05, 0.15)),\n",
    "        keras_cv.layers.RandomRotation(factor=(0.01, 0.05)),\n",
    "    ]\n",
    "    \n",
    "    # Apply augmentations to random samples\n",
    "    aug_layers = [keras_cv.layers.RandomApply(x, rate=0.5) for x in aug_layers]\n",
    "    \n",
    "    # Build augmentation layer\n",
    "    augmenter = keras_cv.layers.Augmenter(aug_layers)\n",
    "\n",
    "    # Apply augmentations\n",
    "    def augment(inp, label):\n",
    "        images = inp[\"images\"]\n",
    "        aug_data = {\"images\": images}\n",
    "        aug_data = augmenter(aug_data)\n",
    "        inp[\"images\"] = aug_data[\"images\"]\n",
    "        return inp, label\n",
    "    return augment\n",
    "\n",
    "\n",
    "def build_decoder(with_labels=True, target_size=CFG.image_size):\n",
    "    def decode_image(inp):\n",
    "        path = inp[\"images\"]\n",
    "        \n",
    "        # Read jpeg image\n",
    "        file_bytes = tf.io.read_file(path)\n",
    "        image = tf.io.decode_jpeg(file_bytes)\n",
    "        \n",
    "        # Resize\n",
    "        image = tf.image.resize(image, size=target_size, method=\"area\")\n",
    "        \n",
    "        # Rescale image\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255.0\n",
    "        \n",
    "        # Reshape\n",
    "        image = tf.reshape(image, [*target_size, 3])\n",
    "        \n",
    "        inp[\"images\"] = image\n",
    "        return inp\n",
    "\n",
    "    def decode_label(label, num_classes):\n",
    "        label = tf.cast(label, tf.float32)\n",
    "        label = tf.reshape(label, [num_classes])\n",
    "        return label\n",
    "\n",
    "    def decode_with_labels(inp, labels=None):\n",
    "        inp = decode_image(inp)\n",
    "        label = decode_label(labels[0], CFG.num_classes)\n",
    "        aux_label = decode_label(labels[1], CFG.aux_num_classes)\n",
    "        return (inp, (label, aux_label))\n",
    "\n",
    "    return decode_with_labels if with_labels else decode_image\n",
    "\n",
    "\n",
    "def build_dataset(\n",
    "    paths,\n",
    "    features,\n",
    "    labels=None,\n",
    "    aux_labels=None,\n",
    "    batch_size=32,\n",
    "    cache=True,\n",
    "    decode_fn=None,\n",
    "    augment_fn=None,\n",
    "    augment=False,\n",
    "    repeat=True,\n",
    "    shuffle=1024,\n",
    "    cache_dir=\"\",\n",
    "    drop_remainder=False,\n",
    "):\n",
    "    if cache_dir != \"\" and cache is True:\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    if decode_fn is None:\n",
    "        decode_fn = build_decoder(labels is not None or aux_labels is not None)\n",
    "\n",
    "    if augment_fn is None:\n",
    "        augment_fn = build_augmenter()\n",
    "\n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    inp = {\"images\": paths, \"features\": features}\n",
    "    slices = (inp, (labels, aux_labels)) if labels is not None else inp\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(slices)\n",
    "    ds = ds.map(decode_fn, num_parallel_calls=AUTO)\n",
    "    ds = ds.cache(cache_dir) if cache else ds\n",
    "    ds = ds.repeat() if repeat else ds\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle, seed=CFG.seed)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        ds = ds.with_options(opt)\n",
    "    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n",
    "    ds = ds.map(augment_fn, num_parallel_calls=AUTO) if augment else ds\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=CFG.num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Create separate bin for each traits\n",
    "for i, trait in enumerate(CFG.class_names):\n",
    "\n",
    "    # Determine the bin edges dynamically based on the distribution of traits\n",
    "    bin_edges = np.percentile(df[trait], np.linspace(0, 100, CFG.num_folds + 1))\n",
    "    df[f\"bin_{i}\"] = np.digitize(df[trait], bin_edges)\n",
    "\n",
    "# Concatenate the bins into a final bin\n",
    "df[\"final_bin\"] = (\n",
    "    df[[f\"bin_{i}\" for i in range(len(CFG.class_names))]]\n",
    "    .astype(str)\n",
    "    .agg(\"\".join, axis=1)\n",
    ")\n",
    "\n",
    "# Perform the stratified split using final bin\n",
    "df = df.reset_index(drop=True)\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(df, df[\"final_bin\"])):\n",
    "    df.loc[valid_idx, \"fold\"] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "for col in CFG.class_names:\n",
    "    lower_quantile = df[col].quantile(0.005)\n",
    "    upper_quantile = df[col].quantile(0.985)\n",
    "    df = df[(df[col] >= lower_quantile) & (df[col] <= upper_quantile)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Sample from full data\n",
    "sample_df = df.copy()\n",
    "train_df = sample_df[sample_df.fold != CFG.fold]\n",
    "valid_df = sample_df[sample_df.fold == CFG.fold]\n",
    "print(f\"# Num Train: {len(train_df)} | Num Valid: {len(valid_df)}\")\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_df[FEATURE_COLS].values)\n",
    "valid_features = scaler.transform(valid_df[FEATURE_COLS].values)\n",
    "\n",
    "# Train\n",
    "train_paths = train_df.image_path.values\n",
    "train_labels = train_df[CFG.class_names].values\n",
    "train_aux_labels = train_df[CFG.aux_class_names].values\n",
    "train_ds = build_dataset(train_paths, train_features, train_labels, train_aux_labels,\n",
    "                         batch_size=CFG.batch_size,\n",
    "                         repeat=True, shuffle=True, augment=True, cache=False)\n",
    "\n",
    "# Valid\n",
    "valid_paths = valid_df.image_path.values\n",
    "valid_labels = valid_df[CFG.class_names].values\n",
    "valid_aux_labels = valid_df[CFG.aux_class_names].values\n",
    "valid_ds = build_dataset(valid_paths, valid_features, valid_labels, valid_aux_labels,\n",
    "                         batch_size=CFG.batch_size,\n",
    "                         repeat=False, shuffle=False, augment=False, cache=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "inps, tars = next(iter(train_ds))\n",
    "imgs = inps[\"images\"]\n",
    "num_imgs, num_cols = 8, 4\n",
    "\n",
    "plt.figure(figsize=(4 * num_cols, num_imgs // num_cols * 5))\n",
    "for i, (img, tar) in enumerate(zip(imgs[:num_imgs], tars[0][:num_imgs])):\n",
    "    plt.subplot(num_imgs // num_cols, num_cols, i + 1)\n",
    "    img = img. numpy()\n",
    "    tar = tar.numpy()\n",
    "    \n",
    "    img = (img - img.min()) / (img.max() + 1e-4)\n",
    "\n",
    "    formatted_tar = \"\\n\".join(\n",
    "        [\n",
    "            \", \".join(\n",
    "                f\"{name.replace('_mean','')}: {val:.2f}\"\n",
    "                for name, val in zip(CFG.class_names[j : j + 3], tar[j : j + 3])\n",
    "            )\n",
    "            for j in range(0, len(CFG.class_names), 3)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"[{formatted_tar}]\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define loss</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class R2Loss(keras.losses.Loss):\n",
    "    def __init__(self, use_mask=False, name=\"r2_loss\"):\n",
    "        super().__init__(name=name)\n",
    "        self.use_mask = use_mask\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        if self.use_mask:\n",
    "            mask = (y_true != -1)\n",
    "            y_true = ops.where(mask, y_true, 0.0)\n",
    "            y_pred = ops.where(mask, y_pred, 0.0)\n",
    "        SS_res = ops.sum(ops.square(y_true - y_pred), axis=0)  # (B, C) -> (C,)\n",
    "        SS_tot = ops.sum(ops.square(y_true - ops.mean(y_true, axis=0)), axis=0)  # (B, C) -> (C,)\n",
    "        r2_loss = SS_res / (SS_tot + 1e-6)  # (C,)\n",
    "        return ops.mean(r2_loss)  # ()\n",
    "    \n",
    "class R2Metric(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"r2\", **kwargs):\n",
    "        super(R2Metric, self).__init__(name=name, **kwargs)\n",
    "        self.SS_res = self.add_weight(name='SS_res', shape=(6,), initializer='zeros')\n",
    "        self.SS_tot = self.add_weight(name='SS_tot', shape=(6,) ,initializer='zeros')\n",
    "        self.num_samples = self.add_weight(name='num_samples', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        SS_res = ops.sum(ops.square(y_true - y_pred), axis=0)\n",
    "        SS_tot = ops.sum(ops.square(y_true - ops.mean(y_true, axis=0)), axis=0)\n",
    "        self.SS_res.assign_add(SS_res)\n",
    "        self.SS_tot.assign_add(SS_tot)\n",
    "        self.num_samples.assign_add(ops.cast(ops.shape(y_true)[0], \"float32\"))\n",
    "\n",
    "    def result(self):\n",
    "        r2 = 1 - self.SS_res / (self.SS_tot + 1e-6)\n",
    "        return ops.mean(r2)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.total_SS_res.assign(0)\n",
    "        self.total_SS_tot.assign(0)\n",
    "        self.num_samples.assign(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Construct model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define input layers\n",
    "img_input = keras.Input(shape=(*CFG.image_size, 3), name=\"images\")\n",
    "feat_input = keras.Input(shape=(len(FEATURE_COLS),), name=\"features\")\n",
    "\n",
    "# Branch for image input\n",
    "backbone = keras_cv.models.EfficientNetV2Backbone.from_preset(CFG.preset)\n",
    "x1 = backbone(img_input)\n",
    "x1 = keras.layers.GlobalAveragePooling2D()(x1)\n",
    "x1 = keras.layers.Dropout(0.2)(x1)\n",
    "\n",
    "# TODO remove these? future test\n",
    "# # Branch for tabular/feature input\n",
    "# x2 = keras.layers.Dense(326, activation=\"selu\")(feat_input)\n",
    "# x2 = keras.layers.Dense(64, activation=\"selu\")(x2)\n",
    "# x2 = keras.layers.Dropout(0.1)(x2)\n",
    "\n",
    "# Concatenate both branches\n",
    "concat = keras.layers.Concatenate()([x1])#, x2])\n",
    "\n",
    "# Output layer\n",
    "out1 = keras.layers.Dense(CFG.num_classes, activation=None, name=\"head\")(concat)\n",
    "out2 = keras.layers.Dense(CFG.aux_num_classes, activation=\"relu\", name=\"aux_head\")(concat)\n",
    "out = {\"head\": out1, \"aux_head\":out2}\n",
    "\n",
    "# Build model\n",
    "model = keras.models.Model([img_input, feat_input], out)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss={\n",
    "        \"head\": R2Loss(use_mask=False),\n",
    "        \"aux_head\": R2Loss(use_mask=True), # use_mask to ignore `NaN` auxiliary labels\n",
    "    },\n",
    "    loss_weights={\"head\": 1.0, \"aux_head\": 0.3},  # more weight to main task\n",
    "    metrics={\"head\": R2Metric()}, # evaluation metric only on main task\n",
    ")\n",
    "\n",
    "# Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LR Scheduler</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):\n",
    "    lr_start, lr_max, lr_min = 5e-5, 8e-6 * batch_size, 1e-5\n",
    "    lr_ramp_ep, lr_sus_ep, lr_decay = 3, 0, 0.75\n",
    "\n",
    "    def lrfn(epoch):  # Learning rate update function\n",
    "        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n",
    "        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n",
    "        elif mode == 'cos':\n",
    "            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n",
    "            phase = math.pi * decay_epoch_index / decay_total_epochs\n",
    "            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n",
    "        return lr\n",
    "\n",
    "    if plot:  # Plot lr curve if plot is True\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n",
    "        plt.xlabel('epoch'); plt.ylabel('lr')\n",
    "        plt.title('LR Scheduler')\n",
    "        plt.show()\n",
    "\n",
    "    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "lr_cb = get_lr_callback(CFG.batch_size, mode=CFG.lr_mode, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.retrain_classifier:\n",
    "    ckpt_cb = keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model.keras\",\n",
    "        monitor=\"val_head_r2\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        mode=\"max\",\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=CFG.epochs,\n",
    "        callbacks=[lr_cb, ckpt_cb],\n",
    "        steps_per_epoch=len(train_df) // CFG.batch_size,\n",
    "        validation_data=valid_ds,\n",
    "        verbose=CFG.verbose,\n",
    "    )\n",
    "\n",
    "    # Best Result\n",
    "    best_R2 = max(history.history['val_head_r2'])\n",
    "    best_Epoch = np.argmax(history.history['val_head_r2']) + 1\n",
    "    print(\"#\" * 10 + \" Result \" + \"#\" * 10)\n",
    "    print(f\"Best R2: {best_R2:.5f}\")\n",
    "    print(f\"Best Epoch: {best_Epoch}\")\n",
    "    print(\"#\" * 28)\n",
    "else:\n",
    "    model.load_weights(\"best_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>XGBoost</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Extract features from data using this model\n",
    "def extract_features(df, model, batch_size=32):\n",
    "    ds = build_dataset(\n",
    "        df.image_path.values,\n",
    "        df[FEATURE_COLS].values,\n",
    "        batch_size=batch_size,\n",
    "        repeat=False,\n",
    "        shuffle=False,\n",
    "        augment=False,\n",
    "        cache=False,\n",
    "    )\n",
    "    features_list = []\n",
    "\n",
    "    for batch_imgs in tqdm(ds):\n",
    "        nondense_model = keras.Model(inputs=model.input, outputs=model.get_layer('concatenate').output)\n",
    "        features = nondense_model.predict(batch_imgs, verbose=0)\n",
    "        features_list.extend(features)\n",
    "\n",
    "    features_array = np.array(features_list)\n",
    "    features_df = pd.DataFrame(features_array)\n",
    "    features_df.columns = [f'feature_{i}' for i in range(features_array.shape[1])]\n",
    "    new_df = pd.concat([df.reset_index(drop=True), features_df.reset_index(drop=True)], axis=1)\n",
    "    return new_df\n",
    "\n",
    "# Extract features from data\n",
    "features = extract_features(df, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "unneeded_columns = ['image_path', \n",
    "                    'final_bin', \n",
    "                    'X11_sd', \n",
    "                    'X3112_sd', \n",
    "                    'bin_1', \n",
    "                    'X50_sd', \n",
    "                    'X4_sd', \n",
    "                    'X26_sd', \n",
    "                    'bin_2', \n",
    "                    'X18_sd', \n",
    "                    'fold', \n",
    "                    'bin_5', \n",
    "                    'bin_3',\n",
    "                    'bin_0', \n",
    "                    'bin_4']\n",
    "\n",
    "X_full = features.drop(columns=CFG.class_names).drop(columns=unneeded_columns)\n",
    "Y_full = features[CFG.class_names]\n",
    "\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_models = {}\n",
    "for column in Y_full.columns:\n",
    "    print(f\"Training {column}\")\n",
    "    xgb_model = XGBRegressor(objective='reg:squarederror', n_estimators=150, learning_rate=0.1, max_depth=10)\n",
    "\n",
    "    xgb_model.fit(X_full, Y_full[column])\n",
    "\n",
    "    if CFG.perform_cv:\n",
    "        print(f\"\\nDoing cross-validation scoring for {column}...\")\n",
    "        scores = cross_val_score(xgb_model, X_full, Y_full[column],\n",
    "                                 cv=KFold(n_splits=3, shuffle=True, random_state=42),\n",
    "                                 scoring='r2', verbose=3)\n",
    "        print(f\"R^2 score for {column}: {np.mean(scores)}\\n\\n\")\n",
    "\n",
    "    xgb_models[column] = xgb_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Results</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Run predictions on test data\n",
    "test_features = extract_features(test_df, model)\n",
    "\n",
    "# Prepare features for XGBoost\n",
    "X_test = test_features.drop(columns=['image_path'])\n",
    "\n",
    "# Make predictions\n",
    "mean_values = Y_full.mean()\n",
    "submission = pd.DataFrame({'id': test_features['id']})\n",
    "for column in CFG.class_names:\n",
    "    xgb_model = xgb_models[column]\n",
    "    submission[column] = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate R2 score\n",
    "r2_scores = []\n",
    "for column in CFG.class_names:\n",
    "    r2_score = xgb_models[column].score(X_full, Y_full[column])\n",
    "    r2_scores.append(r2_score)\n",
    "    print(f\"R^2 score for {column}: {r2_score}\")\n",
    "print(f\"Mean R^2 score: {np.mean(r2_scores)}\")\n",
    "\n",
    "# Save predictions\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
